{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import monpa\n",
    "import pandas as pd\n",
    "def get_dataset(filepath):\n",
    "    sentence = []\n",
    "    ID = []\n",
    "    with open(filepath,'r',encoding='UTF-8') as f:\n",
    "        for line in f.read().splitlines():\n",
    "            items = line.split('\\t')   #用tab隔開\n",
    "            if(len(items)==2):\n",
    "                sentence.append(items[0])\n",
    "                ID.append(items[1])\n",
    "    return sentence ,ID\n",
    "label ,sentence=get_dataset('./E-commerce reviews_chinese.txt')\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    final_data = []\n",
    "    result_cut_batch = []\n",
    "    final_sent = []\n",
    "    final_sentence = []\n",
    "    len_sent = []\n",
    "    for i in sentence:\n",
    "        a = re.sub(r'[^\\u4e00-\\u9fa5]','',i)\n",
    "        final_data.append(a)    \n",
    "    monpa.load_userdict(\"./userdict1.txt\")\n",
    "    for i in final_data:\n",
    "        item = monpa.cut(i)\n",
    "        result_cut_batch.append(item)  \n",
    "    stopwords_data = pd.read_csv(\"./NEW_stopwords.csv\",encoding = 'utf-8')\n",
    "    stopWords = list(stopwords_data[\"stopwords\"])    \n",
    "    for i in result_cut_batch:\n",
    "        items = list(filter(lambda a: a not in stopWords and a != '\\n', i))\n",
    "        final_sent.append(items)\n",
    "    for i in final_sent:\n",
    "        a = len(i)\n",
    "        len_sent.append(a)\n",
    "    for i in range(len(final_sent)):\n",
    "        if len_sent[i] == 0:\n",
    "            yuya= result_cut_batch[i]\n",
    "        else:\n",
    "            yuya=final_sent[i]\n",
    "        final_sentence.append(yuya)\n",
    "    return final_sentence\n",
    "final_sentence = preprocessing(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "________________________________________________________________________________________________________________________\n",
      "Layer (type)                           Output Shape               Param #       Connected to                            \n",
      "========================================================================================================================\n",
      "Input-Token (InputLayer)               (None, 30)                 0                                                     \n",
      "________________________________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)             (None, 30)                 0                                                     \n",
      "________________________________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding)       [(None, 30, 768), (21128,  16226304      Input-Token[0][0]                       \n",
      "________________________________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)          (None, 30, 768)            1536          Input-Segment[0][0]                     \n",
      "________________________________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)          (None, 30, 768)            0             Embedding-Token[0][0]                   \n",
      "                                                                                Embedding-Segment[0][0]                 \n",
      "________________________________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmbedding) (None, 30, 768)            23040         Embedding-Token-Segment[0][0]           \n",
      "________________________________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)            (None, 30, 768)            0             Embedding-Position[0][0]                \n",
      "________________________________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalization)    (None, 30, 768)            1536          Embedding-Dropout[0][0]                 \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Embedding-Norm[0][0]                    \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-1-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Embedding-Norm[0][0]                    \n",
      "                                                                                Encoder-1-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-1-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-1-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-1-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-1-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-1-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-1-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-1-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-2-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-1-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-2-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-2-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-2-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-2-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-2-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-2-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-2-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-2-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-3-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-2-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-3-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-3-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-3-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-3-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-3-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-3-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-3-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-3-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-4-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-3-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-4-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-4-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-4-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-4-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-4-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-4-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-4-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-4-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-5-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-4-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-5-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-5-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-5-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-5-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-5-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-5-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-5-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-5-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-6-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-5-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-6-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-6-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-6-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-6-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-6-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-6-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-6-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-6-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-7-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-6-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-7-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-7-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-7-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-7-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-7-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-7-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-7-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-7-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-8-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-7-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-8-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-8-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-8-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-8-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-8-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-8-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-8-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttention (Mult (None, 30, 768)            2362368       Encoder-8-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttention-Dropo (None, 30, 768)            0             Encoder-9-MultiHeadSelfAttention[0][0]  \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttention-Add ( (None, 30, 768)            0             Encoder-8-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-9-MultiHeadSelfAttention-Dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttention-Norm  (None, 30, 768)            1536          Encoder-9-MultiHeadSelfAttention-Add[0][\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForward)    (None, 30, 768)            4722432       Encoder-9-MultiHeadSelfAttention-Norm[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout (Dropout (None, 30, 768)            0             Encoder-9-FeedForward[0][0]             \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add)        (None, 30, 768)            0             Encoder-9-MultiHeadSelfAttention-Norm[0]\n",
      "                                                                                Encoder-9-FeedForward-Dropout[0][0]     \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (LayerNorma (None, 30, 768)            1536          Encoder-9-FeedForward-Add[0][0]         \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttention (Mul (None, 30, 768)            2362368       Encoder-9-FeedForward-Norm[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttention-Drop (None, 30, 768)            0             Encoder-10-MultiHeadSelfAttention[0][0] \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttention-Add  (None, 30, 768)            0             Encoder-9-FeedForward-Norm[0][0]        \n",
      "                                                                                Encoder-10-MultiHeadSelfAttention-Dropou\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttention-Norm (None, 30, 768)            1536          Encoder-10-MultiHeadSelfAttention-Add[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedForward)   (None, 30, 768)            4722432       Encoder-10-MultiHeadSelfAttention-Norm[0\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout (Dropou (None, 30, 768)            0             Encoder-10-FeedForward[0][0]            \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add)       (None, 30, 768)            0             Encoder-10-MultiHeadSelfAttention-Norm[0\n",
      "                                                                                Encoder-10-FeedForward-Dropout[0][0]    \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (LayerNorm (None, 30, 768)            1536          Encoder-10-FeedForward-Add[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttention (Mul (None, 30, 768)            2362368       Encoder-10-FeedForward-Norm[0][0]       \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttention-Drop (None, 30, 768)            0             Encoder-11-MultiHeadSelfAttention[0][0] \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttention-Add  (None, 30, 768)            0             Encoder-10-FeedForward-Norm[0][0]       \n",
      "                                                                                Encoder-11-MultiHeadSelfAttention-Dropou\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttention-Norm (None, 30, 768)            1536          Encoder-11-MultiHeadSelfAttention-Add[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedForward)   (None, 30, 768)            4722432       Encoder-11-MultiHeadSelfAttention-Norm[0\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout (Dropou (None, 30, 768)            0             Encoder-11-FeedForward[0][0]            \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add)       (None, 30, 768)            0             Encoder-11-MultiHeadSelfAttention-Norm[0\n",
      "                                                                                Encoder-11-FeedForward-Dropout[0][0]    \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (LayerNorm (None, 30, 768)            1536          Encoder-11-FeedForward-Add[0][0]        \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttention (Mul (None, 30, 768)            2362368       Encoder-11-FeedForward-Norm[0][0]       \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttention-Drop (None, 30, 768)            0             Encoder-12-MultiHeadSelfAttention[0][0] \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttention-Add  (None, 30, 768)            0             Encoder-11-FeedForward-Norm[0][0]       \n",
      "                                                                                Encoder-12-MultiHeadSelfAttention-Dropou\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttention-Norm (None, 30, 768)            1536          Encoder-12-MultiHeadSelfAttention-Add[0]\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedForward)   (None, 30, 768)            4722432       Encoder-12-MultiHeadSelfAttention-Norm[0\n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout (Dropou (None, 30, 768)            0             Encoder-12-FeedForward[0][0]            \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add)       (None, 30, 768)            0             Encoder-12-MultiHeadSelfAttention-Norm[0\n",
      "                                                                                Encoder-12-FeedForward-Dropout[0][0]    \n",
      "________________________________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (LayerNorm (None, 30, 768)            1536          Encoder-12-FeedForward-Add[0][0]        \n",
      "========================================================================================================================\n",
      "Total params: 101,306,880\n",
      "Trainable params: 0\n",
      "Non-trainable params: 101,306,880\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "import numpy as np\n",
    "import codecs\n",
    "from keras_bert import Tokenizer\n",
    "MAX_SEQ_LEN = 30\n",
    "BERT_PRETRAINED_DIR = './chinese_L-12_H-768_A-12' # need to rewrite if you put model in other directory\n",
    "config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "model = load_trained_model_from_checkpoint(config_file, checkpoint_file, seq_len=MAX_SEQ_LEN)\n",
    "model.summary(line_length=120)\n",
    "def get_bert_embedding2 (word):\n",
    "    dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    token_dict = {}\n",
    "    with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "       \n",
    "    tokenizer = Tokenizer(token_dict)\n",
    "    bert=[]\n",
    "    for i in word:\n",
    "        tokens = tokenizer.tokenize(i)\n",
    "    #     print('Tokens:', tokens)\n",
    "    #     print('==============================================================')\n",
    "        indices, segments = tokenizer.encode(first=i, max_len=MAX_SEQ_LEN)\n",
    "    # extract vectors\n",
    "        predicts = model.predict([np.array([indices]), np.array([segments])])[0]\n",
    "        bert.append(predicts[0].tolist()) # 僅列出前5個向量值\n",
    "    return bert\n",
    "total_bert = get_bert_embedding2(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\"\"\"\n",
    "total_V_ans and total_A_ans are the Valence and Arousal of each word\n",
    "Please refer to page 7 in our paper to know the detail for calculating the Valence and Arousal.\n",
    "\"\"\"\n",
    "\n",
    "def top3VA(final_sentence,final_LLRsentence,total_V_ans,total_A_ans,total_dict):\n",
    "    total_word = []\n",
    "    nnV = []\n",
    "    nnV1 = []\n",
    "    for i in final_sentence:\n",
    "        for item in i:\n",
    "            total_word.append(item)\n",
    "    total_word = to_total(final_sentence)\n",
    "    total_V = to_total(total_V_ans)\n",
    "    total_A = to_total(total_A_ans)\n",
    "    totalwordV_dict = dict(zip(total_word,total_V))\n",
    "    totalwordA_dict = dict(zip(total_word,total_A))\n",
    "    for i in total_dict.keys():\n",
    "        for item in totalwordV_dict:\n",
    "            if i == item:\n",
    "                nnV.append(totalwordV_dict[item])\n",
    "                nnV1.append(i)\n",
    "    total_dictvalue = []\n",
    "    for i in nnV1:\n",
    "        a = total_dict.get(i)\n",
    "        total_dictvalue.append(a)\n",
    "    nnA = []\n",
    "    for i in total_dict.keys():\n",
    "        for item in totalwordA_dict:\n",
    "            if i == item:\n",
    "                nnA.append(totalwordA_dict[item])\n",
    "    LLR_dictVA = list(zip(nnV1,total_dictvalue,nnV,nnA))\n",
    "    original_ssent = list(zip(total_word,total_V,total_A))\n",
    "    top3VA = []\n",
    "    avvv = []\n",
    "    for i in range(len(final_LLRsentence)):\n",
    "        for u in final_LLRsentence[i]:\n",
    "            xx =[]\n",
    "            for item in LLR_dictVA:\n",
    "                if len(final_LLRsentence[i])>3:\n",
    "                    if u == item[0]:\n",
    "                        avvv.append(item[1])\n",
    "                        a = heapq.nlargest(3,avvv)\n",
    "                        for ig in a:\n",
    "                            b = [y[1] for y in LLR_dictVA].index(ig)\n",
    "                            final_v = LLR_dictVA[b][2]\n",
    "                            final_v1 = LLR_dictVA[b][3]\n",
    "                            xx.append([final_v,final_v1])\n",
    "                elif len(final_LLRsentence[i])==3:\n",
    "                    if u == item[0]:\n",
    "                        for ig in final_LLRsentence[i]:\n",
    "                            b = [y[0] for y in LLR_dictVA].index(ig)\n",
    "                            final_v = LLR_dictVA[b][2]\n",
    "                            final_v1 = LLR_dictVA[b][3]\n",
    "                            xx.append([final_v,final_v1])\n",
    "                elif len(final_LLRsentence[i])<3: \n",
    "                    if u == item[0]:\n",
    "                        for ig in final_LLRsentence[i]:\n",
    "                            b = [y[0] for y in LLR_dictVA].index(ig)\n",
    "                            final_v = LLR_dictVA[b][2]\n",
    "                            final_v1 = LLR_dictVA[b][3]\n",
    "                            xx.append([final_v,final_v1])\n",
    "                        if final_sentence[i] != item[0]:                            \n",
    "                            k = heapq.nlargest(3-len(final_LLRsentence[i]),total_V_ans[i])\n",
    "                            for ll in k:\n",
    "                                b1 = [y[1] for y in original_ssent].index(ll)\n",
    "                                final_a = original_ssent[b1][1]\n",
    "                                final_a1 = original_ssent[b1][2]\n",
    "                                xx.append([final_a,final_a1])\n",
    "        top3VA.append(xx)\n",
    "    final_top3va = []\n",
    "    final_top3VA = []\n",
    "    for i in top3VA:\n",
    "        a = []\n",
    "        for item in i:\n",
    "            for items in item:\n",
    "                a.append(items)\n",
    "        final_top3va.append(a)\n",
    "    for i in final_top3va:\n",
    "        b = list(i + [0] * (6 - len(i)))\n",
    "        final_top3VA.append(b)\n",
    "    return final_top3VA\n",
    "final_top3VA = top3VA(final_sentence,final_LLRsentence,total_V_ans,total_A_ans,total_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sentiment_key_word():\n",
    "    \"\"\"\n",
    "    In order to better identify the sentiment of the messages, \n",
    "    Log-Likelihood Ratio (LLR) estimation is introduced in this research \n",
    "    to score the most representative words for the positive and negative categories. \n",
    "    \"\"\"\n",
    "    return pos_keyword,neg_keyword\n",
    "\n",
    "pos_keyword,neg_keyword = get_sentiment_key_word()\n",
    "total_dict = dict(pos_keyword, **neg_keyword)\n",
    "def ComparisonLLR(final_sentence):\n",
    "    final_LLRsentence = []\n",
    "    for i in final_sentence:\n",
    "        a = []\n",
    "        for item in i:\n",
    "            if item in total_dict:\n",
    "                a.append(item)\n",
    "        final_LLRsentence.append(a)\n",
    "    return final_LLRsentence\n",
    "final_LLRsentence = ComparisonLLR(final_sentence)\n",
    "def LLR200(final_sentence,pos_keyword,neg_keyword):\n",
    "    pos_nn = []\n",
    "    neg_nn = []\n",
    "    for item in final_sentence:\n",
    "        a = []\n",
    "        for i in pos_keyword.keys():\n",
    "            if i in item:\n",
    "                a.append(pos_keyword[i])\n",
    "            else:\n",
    "                a.append(0)\n",
    "        pos_nn.append(a)\n",
    "    for item in final_sentence:\n",
    "        b = []\n",
    "        for i in neg_keyword.keys():\n",
    "            if i in item:\n",
    "                b.append(neg_keyword[i])\n",
    "            else:\n",
    "                b.append(0)\n",
    "        neg_nn.append(b)\n",
    "    return pos_nn,neg_nn\n",
    "pos_nn,neg_nn = LLR200(final_sentence,pos_keyword,neg_keyword)\n",
    "\n",
    "#concatenate\n",
    "final_input = np.concatenate((total_bert,final_top3VA,pos_nn,neg_nn),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import activations\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Bidirectional\n",
    "K.clear_session()\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, attention_size=None, **kwargs):\n",
    "        self.attention_size = attention_size\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['attention_size'] = self.attention_size\n",
    "        return config\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.time_steps = input_shape[1]\n",
    "        hidden_size = input_shape[2]\n",
    "        if self.attention_size is None:\n",
    "            self.attention_size = hidden_size\n",
    "            \n",
    "        self.W = self.add_weight(name='att_weight', shape=(hidden_size, self.attention_size),\n",
    "                                initializer='uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(self.attention_size,),\n",
    "                                initializer='uniform', trainable=True)\n",
    "        self.V = self.add_weight(name='att_var', shape=(self.attention_size,),\n",
    "                                initializer='uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.V = K.reshape(self.V, (-1, 1))\n",
    "        H = K.tanh(K.dot(inputs, self.W) + self.b)\n",
    "        score = K.softmax(K.dot(H, self.V), axis=1)\n",
    "        outputs = K.sum(score * inputs, axis=1)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape, Flatten\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional, BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "predicted= []\n",
    "expected= []\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True) \n",
    "cv.get_n_splits(final_input,label)\n",
    "for train_idx, test_idx in cv.split(final_input,label):\n",
    "    x_train = np.array(final_input)[train_idx]\n",
    "    y_train = np.array(label)[train_idx]\n",
    "    x_test = np.array(final_input)[test_idx]\n",
    "    y_test = np.array(label)[test_idx] \n",
    "    x_train1 = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    x_test1 = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128,return_sequences=True,input_shape=(1,final_input.shape[1]))))\n",
    "    model.add(Activation('relu',name='attention_weights'))\n",
    "    model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(Dense(units = 64, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(units = 32, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    filepath=\"weights_waimai.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True,mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    print(model.summary())\n",
    "    model.fit(x_train1, y_train, epochs=10, batch_size=32,callbacks=callbacks_list, verbose=1)\n",
    "    expected.extend(y_test)\n",
    "    predicted.extend(model.predict_classes(x_test1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
